# -*- coding: utf-8 -*-
"""(Samuel Wahome) Streamlit Checkpoint 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15igVB0T89OnwwIJDuiykYzS8F-G0CeoZ

# **Instructions**

1. Install the necessary packages
2. Import you data and perform basic data exploration phase
  - Display general information about the dataset
  - Create a pandas profiling reports to gain insights into the dataset
  - Handle Missing and corrupted values
  - Remove duplicates, if they exist
  - Handle outliers, if they exist
  - Encode categorical features
3. Based on the previous data exploration train and test a machine learning classifier
4. Create a streamlit application.
5. Add input fields for your features and a validation button at the end of the form
6. Import your ML model into the streamlit application and start making predictions given the provided features values

# **1: Load and Explore the Dataset**

## ***A. Display general information about the dataset***`
"""

import pandas as pd
from IPython.display import display, HTML

# Load the dataset
file_path = '/content/drive/MyDrive/Colab Notebooks/Expresso_churn_dataset.csv'
data = pd.read_csv(file_path)

# Create a copy of the original data
original_data = data.copy()

# Display general information about the dataset
info_html = data.info(buf=None)  # No need to store output as it prints directly

# Display the first few rows using HTML
head_html = data.head().to_html()

# Display summary statistics using HTML
describe_html = data.describe().to_html()

# Output the information and tables
display(HTML("<h3>Dataset Information</h3>"))
display(HTML(info_html))
display(HTML("<h3>First Few Rows of the Dataset</h3>"))
display(HTML(head_html))
display(HTML("<h3>Summary Statistics</h3>"))
display(HTML(describe_html))

"""## ***B. Create a pandas profiling reports to gain insights into the dataset***
Let's generate the pandas profiling report for more insights.
"""

!pip install ydata-profiling --upgrade

# Import necessary libraries
import pandas as pd
from ydata_profiling import ProfileReport
from google.colab import files

# Generate a profiling report to display in the notebook
profile_report = ProfileReport(data, title="Pandas Profiling Report: Electric Vehicle Data", explorative=True)
profile_report.to_notebook_iframe()

# Generate the profiling report and save to file
profile_rep_gen = ProfileReport(data, title="Pandas Profiling Report: Expresso Churn")
report_path = "/content/drive/MyDrive/Colab Notebooks/Expresso _Churn_Data_Profiling_Report.html"
profile_rep_gen.to_file(report_path)

# Display a link to view the saved HTML file
files.view(report_path)

print(f"The profiling report has been saved to: {report_path}")

"""## ***C. Find Columns with Missing Values***
The next thing is to identify and handle any missing or corrupted values in the dataset.This script does the following. We create a list of dictionaries containing information about missing values. So the script will provide a nicely formatted report of missing values


"""

import pandas as pd
import numpy as np
import io
from IPython.display import display, HTML

# Find columns with missing values
columns_with_missing = data.columns[data.isnull().any()].tolist()

# Prepare results
results = []
if columns_with_missing:
    for column in columns_with_missing:
        missing_count = data[column].isnull().sum()
        missing_percentage = (missing_count / len(data)) * 100
        results.append({
            'Column': column,
            'Missing Values': missing_count,
            'Percentage': f"{missing_percentage:.2f}%"
        })

    # Create a DataFrame for display
    missing_df = pd.DataFrame(results)

    # Display the results
    display(HTML("<h3>Columns with Missing Values:</h3>"))
    display(missing_df)
else:
    display(HTML("<p>No columns have missing values.</p>"))

# Display total number of missing values in the dataset
total_missing = data.isnull().sum().sum()
display(HTML(f"<p><strong>Total missing values in the dataset:</strong> {total_missing}</p>"))

# Display columns with no missing values
columns_without_missing = data.columns[~data.isnull().any()].tolist()
display(HTML("<h3>Columns without Missing Values:</h3>"))
display(HTML("<p>" + ", ".join(columns_without_missing) + "</p>"))

# Optional: Display overall dataset info
display(HTML("<h3>Dataset Info:</h3>"))
buffer = io.StringIO()
data.info(buf=buffer)
info_string = buffer.getvalue()
display(HTML("<pre>" + info_string + "</pre>"))

"""### **Handling Missing Values**

**Impute Missing Values:**

1. **Categorical Columns** (`REGION`, `ZONE1`, `ZONE2`, `TOP_PACK`):
   - Fill missing values with the mode (most frequent value).

2. **Numerical Columns** (`MONTANT`, `FREQUENCE_RECH`, `REVENUE`, `ARPU_SEGMENT`, `FREQUENCE`, `DATA_VOLUME`, `ON_NET`, `ORANGE`, `TIGO`, `FREQ_TOP_PACK`):
   - Fill missing values with the median value.

3. **Remove Rows with Excessive Missing Values**:
   - Columns with a very high percentage of missing values like `ZONE1` (92.12%) and `ZONE2` (93.65%) may be dropped if they do not contribute significantly to the model.
   
### Columns with Missing Values:
- `REGION`: 39.43%
- `MONTANT`: 35.13%
- `FREQUENCE_RECH`: 35.13%
- `REVENUE`: 33.71%
- `ARPU_SEGMENT`: 33.71%
- `FREQUENCE`: 33.71%
- `DATA_VOLUME`: 49.23%
- `ON_NET`: 36.52%
- `ORANGE`: 41.56%
- `TIGO`: 59.89%
- `ZONE1`: 92.12%
- `ZONE2`: 93.65%
- `TOP_PACK`: 41.90%
- `FREQ_TOP_PACK`: 41.90%

### Total missing values in the dataset: 14,380,032

### Columns without Missing Values:
- `user_id`, `TENURE`, `MRG`, `REGULARITY`, `CHURN`
"""

import pandas as pd
from IPython.display import display, HTML

# Identify missing values in each column
missing_values = data.isnull().sum()
missing_percentage = (missing_values / len(data)) * 100

# Display missing value percentages
display(HTML("<h3>Missing values and their percentages before handling:</h3>"))
display(HTML(missing_percentage.to_frame().to_html()))

# Define categorical and numerical columns that need handling
categorical_columns = ['REGION', 'ZONE1', 'ZONE2', 'TOP_PACK']
numerical_columns = ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'TIGO', 'FREQ_TOP_PACK']

# Create a new DataFrame for the cleaned data
cleaned_data = data.copy()

# Threshold to drop columns (e.g., 90% missing values)
threshold = 90.0

# List to store removed columns
removed_columns = []

# Handling missing values based on percentage
for column in cleaned_data.columns:
    if missing_percentage[column] > 0:
        if missing_percentage[column] > threshold:
            # Store column name and drop column
            removed_columns.append(column)
            cleaned_data.drop(columns=[column], inplace=True)
        else:
            # Handle numerical columns
            if column in numerical_columns:
                cleaned_data[column].fillna(cleaned_data[column].median(), inplace=True)
            # Handle categorical columns
            elif column in categorical_columns:
                cleaned_data[column].fillna(cleaned_data[column].mode()[0], inplace=True)

# Display the columns that were removed
if removed_columns:
    display(HTML("<h3>Columns Removed Due to Excessive Missing Values:</h3>"))
    display(HTML(pd.DataFrame(removed_columns, columns=["Removed Columns"]).to_html()))
else:
    display(HTML("<p>No columns were removed.</p>"))

# Verify if there are any missing values left in the cleaned_data
missing_values_after = cleaned_data.isnull().sum()
display(HTML("<h3>Missing values after handling:</h3>"))
display(HTML(missing_values_after.to_frame().to_html()))

# Display the first few rows of the cleaned dataset
display(HTML("<h3>First Few Rows of Cleaned Data:</h3>"))
display(cleaned_data.head())

# Print shape of the cleaned dataset
print(f"Shape of cleaned data: {cleaned_data.shape}")

"""## ***D. Handle Outliers***
Next, we'll check for outliers in the numerical columns using the interquartile range (IQR) method and handle them if necessary.

1. Identification:
   We use three methods to identify outliers:
   - Interquartile Range (IQR): Points beyond 1.5 * IQR from Q1 and Q3
   - Z-score: Points more than 3 standard deviations from the mean
   - Modified Z-score: Uses median absolute deviation, robust to extreme outliers

2. Visualization:
   We plot the data points and highlight the outliers identified by each method.
   This helps us visually assess the distribution and the extent of outliers.

3. Handling Method:
   We use the 'cap' method to handle outliers:
   - Cap: Limits values to the 1st and 99th percentiles
   This preserves the data points while reducing the impact of extreme values.

4. Before and After Comparison:
   We visualize the data and provide summary statistics before and after
   handling outliers to show the effect of our outlier treatment.

This approach allows us to identify and mitigate the impact of outliers
while preserving the overall structure of the data for our clustering analysis.

**Methodology:**
1. Numeric columns: Numeric columns of the dataset are identified.
2. Histograms: Histograms for each numeric column are created.
3. Q-Q plots:
4. Handling outliers: Outliers in each numeric column are handled using the 'cap' method.
5. Plotting outliers after handling: Outliers for each column are plotted after handling them to visualize the impact of the outlier handling.

### **1. Load the set & Define Helper Functions**

These functions are designed to identify, plot, and handle outliers using multiple methods.efine functions.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Functions to identify, plot, and handle outliers
def identify_outliers(cleaned_data, column):
    Q1 = cleaned_data[column].quantile(0.25)
    Q3 = cleaned_data[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    iqr_outliers = cleaned_data[(cleaned_data[column] < lower_bound) | (cleaned_data[column] > upper_bound)]

    z_scores = np.abs(stats.zscore(cleaned_data[column].dropna()))
    z_score_outliers = cleaned_data[z_scores > 3]

    median = cleaned_data[column].median()
    mad = np.median(np.abs(cleaned_data[column] - median))
    modified_z_scores = 0.6745 * (cleaned_data[column] - median) / mad
    modified_z_score_outliers = cleaned_data[np.abs(modified_z_scores) > 3.5]

    return iqr_outliers, z_score_outliers, modified_z_score_outliers

def plot_outliers(cleaned_data, column):
    plt.figure(figsize=(12, 6))
    plt.scatter(cleaned_data.index, cleaned_data[column], alpha=0.5)
    iqr, z, mod_z = identify_outliers(cleaned_data, column)
    plt.scatter(iqr.index, iqr[column], color='red', label='IQR')
    plt.scatter(z.index, z[column], color='green', label='Z-score')
    plt.scatter(mod_z.index, mod_z[column], color='orange', label='Modified Z-score')
    plt.title(f'Outliers in {column}')
    plt.xlabel('Index')
    plt.ylabel(column)
    plt.legend()
    plt.show()

def handle_outliers(cleaned_data, column, method='cap'):
    _, _, outliers = identify_outliers(cleaned_data, column)
    if method == 'cap':
        lower = cleaned_data[column].quantile(0.01)
        upper = cleaned_data[column].quantile(0.99)
        cleaned_data[column] = cleaned_data[column].clip(lower, upper)
    elif method == 'remove':
        cleaned_data = cleaned_data[~cleaned_data.index.isin(outliers.index)]
    return cleaned_data

"""1. `identify_outliers` function: This function identifies outliers in a given column using three different methods:

  - **IQR method:** Outliers are identified based on the interquartile range (IQR). Data points below the lower bound or above the upper bound are considered outliers.
  - **Z-score method:** Outliers are identified based on the Z-score, which measures the number of standard deviations a data point is from the mean. Data points with a Z-score greater than 3 are considered outliers.
  - **Modified Z-score method:** This method is more robust to extreme outliers and uses the median and median absolute deviation (MAD). Data points with a modified Z-score greater than 3.5 are considered outliers.

The function returns three DataFrames, each containing the outliers identified by one of the methods.

2. `plot_outliers `function: This function plots outliers in a given column using the three methods described above. It creates a scatter plot of the data points, with outliers highlighted in different colors for each method (IQR in red, Z-score in green, Modified Z-score in orange).

3. `handle_outliers` function: This function handles outliers in a given column by either capping or removing them:

  - Capping method: Outliers are capped to the 1st and 99th percentiles of the data, effectively reducing the impact of extreme values without removing data points.
  - Removing method: Outliers are removed from the DataFrame entirely.
  
The function returns the DataFrame with outliers handled according to the specified method.

### **2. Exploratory Data Analysis (EDA):**
Here, we need to understand the distribution and characteristics of the data. It provides a summary of the dataset, creates histograms, and generates Q-Q plots for each numeric column. This helps in understanding the data distribution and potential outliers.
"""

import numpy as np
import pandas as pd
from IPython.display import display, HTML

# Create a copy of cleaned_data for outlier handling
data_for_outliers = cleaned_data.copy()

# Print descriptive statistics
descriptive_stats = data_for_outliers.describe()

# Display descriptive statistics using IPython.display
display(HTML("<h3>Descriptive Statistics:</h3>"))
display(HTML(descriptive_stats.to_html()))

# Get numeric columns
numeric_columns = data_for_outliers.select_dtypes(include=[np.number]).columns
n_cols = len(numeric_columns)

print('\n')
print(f"Number of numeric columns: {n_cols}")

"""### **3. Create Visualisations**
Now we will display histograms for all numeric columns in the data_for_outliers DataFrame, showing the distribution of each column's values.

#### **Histograms**
"""

import matplotlib.pyplot as plt
from IPython.display import display, HTML

# Calculate number of rows and columns for subplots
n_cols = len(numeric_columns)
n_rows = (n_cols + 2) // 3
n_cols_plot = min(3, n_cols)

# Create histograms
fig, axes = plt.subplots(n_rows, n_cols_plot, figsize=(5 * n_cols_plot, 4 * n_rows))
fig.suptitle('Histograms of Numeric Columns', fontsize=16)

for i, column in enumerate(numeric_columns):
    row = i // 3
    col = i % 3
    ax = axes[row, col] if n_rows > 1 else axes[col]
    ax.hist(data_for_outliers[column], bins=30)
    ax.set_title(column)
    ax.tick_params(axis='both', which='major', labelsize=8)

# Remove any unused subplots
for i in range(n_cols, n_rows * n_cols_plot):
    row = i // 3
    col = i % 3
    fig.delaxes(axes[row, col] if n_rows > 1 else axes[col])

plt.tight_layout()
plt.subplots_adjust(top=0.9, hspace=0.4, wspace=0.3)
plt.show()

"""#### **Q-Plots**
Next we create Q-Q plots for each numeric column in the data_for_outliers DataFrame. Each plot will help visualize how the data deviates from a normal distribution. Q-Q plots for each numeric column are created to assess normality.
"""

import matplotlib.pyplot as plt
import scipy.stats as stats

# Calculate number of rows and columns for subplots
n_cols = len(numeric_columns)
n_rows = (n_cols + 2) // 3
n_cols_plot = min(3, n_cols)

# Create Q-Q plots
fig, axes = plt.subplots(n_rows, n_cols_plot, figsize=(5 * n_cols_plot, 4 * n_rows))
fig.suptitle('Q-Q Plots of Numeric Columns', fontsize=16, y=1.05)  # Adjust y for title space

# If axes is not a list, convert it to a list for consistent iteration
if n_rows == 1 and n_cols_plot == 1:
    axes = [axes]
elif n_rows == 1:
    axes = axes
else:
    axes = axes.flatten()  # Flatten axes array for easy iteration

# Plot Q-Q plots
for i, column in enumerate(numeric_columns):
    ax = axes[i]
    stats.probplot(data_for_outliers[column], dist="norm", plot=ax)  # Plot directly on ax
    ax.set_title(f"Q-Q Plot of {column}")
    ax.tick_params(axis='both', which='major', labelsize=8)

# Remove any unused subplots
for i in range(len(numeric_columns), len(axes)):
    fig.delaxes(axes[i])

# Adjust layout to avoid overlap
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.subplots_adjust(top=0.88, hspace=0.4, wspace=0.3)

# Display the figure only once
plt.show()

"""#### **Identify & Plot Outliers**
Here we identify and visualize outliers for each numeric column using multiple methods.
"""

import matplotlib.pyplot as plt
from IPython.display import display, HTML

# Loop through each numeric column to identify and plot outliers
for column in numeric_columns:
    # Identify outliers using different methods
    iqr_outliers, z_score_outliers, modified_z_score_outliers = identify_outliers(data_for_outliers, column)

    # Display the number of outliers found by each method
    outlier_info = f"""
    <h3>Outliers in {column}:</h3>
    <ul>
        <li><strong>IQR method:</strong> {len(iqr_outliers)}</li>
        <li><strong>Z-score method:</strong> {len(z_score_outliers)}</li>
        <li><strong>Modified Z-score method:</strong> {len(modified_z_score_outliers)}</li>
    </ul>
    """
    display(HTML(outlier_info))

    # Plot outliers for the column
    plt.figure(figsize=(12, 6))
    plot_outliers(data_for_outliers, column)
    plt.show()

"""### **4. Handle Outliers & Compare Results**
**The Cap Method**

Instead of removing outliers, I decided to cap them at the 1st and 99th percentiles. It preserves data points while mitigating the impact of extreme values.
"""

import pandas as pd
from IPython.display import display, HTML

data_no_outliers = data_for_outliers.copy()

for column in numeric_columns:
    data_no_outliers = handle_outliers(data_no_outliers, column, method='cap')

# Compare shapes
shapes = [
    ["Original", data_for_outliers.shape[0], data_for_outliers.shape[1]],
    ["After handling outliers", data_no_outliers.shape[0], data_no_outliers.shape[1]]
]
shapes_df = pd.DataFrame(shapes, columns=["Dataset", "Rows", "Columns"])
display(HTML("<h3>Comparison of Shapes</h3>"))
display(HTML(shapes_df.to_html(index=False, border=0)))

# Compare summary statistics
stats_before = data_for_outliers.describe().T
stats_after = data_no_outliers.describe().T

display(HTML("<h3>Summary Statistics Before Outlier Handling</h3>"))
display(HTML(stats_before.to_html(float_format="{:.2f}".format, border=0)))

display(HTML("<h3>Summary Statistics After Outlier Handling</h3>"))
display(HTML(stats_after.to_html(float_format="{:.2f}".format, border=0)))

# Count the number of changes
changes = (data_for_outliers != data_no_outliers).sum()
changes_df = pd.DataFrame({"Column": changes.index, "Changes": changes.values})
display(HTML("<h3>Number of Values Changed in Each Column</h3>"))
display(HTML(changes_df.to_html(index=False, border=0)))

display(HTML("<p><strong>Note:</strong> The original 'cleaned_data' DataFrame remains unchanged.</p>"))
display(HTML("<p>Use 'data_no_outliers' for further analysis with outliers handled.</p>"))

"""## ***E. Encode Categorical Features:***
The Code does the following:

1. Checks and displays information about the original cleaned_data.
2. Identifies categorical columns and performs label encoding.
3. Creates a new encoded_data DataFrame with the encoded values.
4. Provides detailed diagnostic information about both the original and encoded data.
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from IPython.display import display, HTML

# Diagnostic step: Check the data after outlier handling
display(HTML("<h3>Data after Outlier Handling</h3>"))
display(HTML(f"<p>Shape: {data_no_outliers.shape}</p>"))

display(HTML("<h4>Column Names:</h4>"))
display(pd.DataFrame(data_no_outliers.columns, columns=['Column Names']))

display(HTML("<h4>Data Types:</h4>"))
display(pd.DataFrame(data_no_outliers.dtypes, columns=['Data Type']))

display(HTML("<h4>First Few Rows of Data:</h4>"))
display(data_no_outliers.head())

# Identify categorical columns
categorical_columns = data_no_outliers.select_dtypes(include=['object']).columns

# Create a new DataFrame for encoded data
encoded_data = data_no_outliers.copy()

# Use Label Encoding for categorical variables
label_encoders = {}
for col in categorical_columns:
    le = LabelEncoder()
    encoded_data[col] = le.fit_transform(data_no_outliers[col].astype(str))
    label_encoders[col] = le

# Encoding process completion message
display(HTML("<h3>Encoding Process Complete</h3>"))
display(HTML("<p>========================</p>"))

# Display information about encoded columns
display(HTML("<h4>Categorical Columns Encoded:</h4>"))
display(pd.DataFrame(categorical_columns, columns=['Encoded Columns']))

# Display shape of dataframes
display(HTML(f"<p>Shape of data after outlier handling: {data_no_outliers.shape}</p>"))
display(HTML(f"<p>Shape of encoded data: {encoded_data.shape}</p>"))

# Diagnostic steps for encoded_data
display(HTML("<h4>Diagnostic Information for encoded_data:</h4>"))
display(HTML(f"<p>Number of rows: {len(encoded_data)}</p>"))
display(HTML(f"<p>Number of columns: {len(encoded_data.columns)}</p>"))

display(HTML("<h4>Column Names of encoded_data:</h4>"))
display(pd.DataFrame(encoded_data.columns, columns=['Column Names']))

display(HTML("<h4>Data Types of Columns in encoded_data:</h4>"))
display(pd.DataFrame(encoded_data.dtypes, columns=['Data Type']))

display(HTML("<h4>First Few Rows of Encoded Data:</h4>"))
display(encoded_data.head())

display(HTML("<h4>Sample of Encoded Data (first 5 rows, first 5 columns):</h4>"))
display(encoded_data.iloc[:5, :5])

# Check if the encoded data is empty
if encoded_data.empty:
    display(HTML("<p style='color: red;'>Warning: encoded_data is empty!</p>"))
else:
    display(HTML("<p>encoded_data contains data.</p>"))

display(HTML("<h4>Note:</h4>"))
display(HTML("<p>'data_no_outliers' remains unchanged. Use the 'encoded_data' frame for the model.</p>"))

"""# **2: Select Target Variable & the Features**

## 1. Target Variable:

The target variable for our analysis is `CHURN`. This binary variable indicates whether a customer has churned or not, making it the appropriate target for our classification task.

## 2. Features:

We have identified several features that may influence customer churn. These include both numeric and categorical features:

### **Numeric Features:**

- `MONTANT` (Amount): The total amount spent by the customer, which could reflect their engagement level.
- `FREQUENCE_RECH` (Search Frequency): How often the customer searches, indicating their activity level.
- `REVENUE` (Revenue): The total revenue generated from the customer.
- `ARPU_SEGMENT` (Average Revenue per User Segment): The average revenue per user segment.
- `FREQUENCE` (Frequency): How often the customer uses the service.
- `DATA_VOLUME` (Data Volume): The volume of data used by the customer.
- `ON_NET` (On-net Calls): The number of calls made on-net, which might reflect usage patterns.
- `ORANGE` (Orange Calls): The number of calls made to Orange, indicating the preference or usage of this network.
- `TIGO` (Tigo Calls): The number of calls made to Tigo, indicating usage patterns.

### **Categorical Features:**

- `REGION`: The geographical region of the customer, which may affect churn due to regional service quality or competition.
- `TENURE`: The customer's tenure with the service provider, which could influence their likelihood of churning.
- `MRG`: The margin or customer segment classification.
- `TOP_PACK`: The top pack or service plan the customer uses.

### **Features to Potentially Exclude:**

- `user_id`: Unique identifier for the customer, not predictive of churn.
- `MRG`: Depending on its relevance and overlap with other features, it might be redundant or not directly predictive.

**Note:** The dataset has been cleaned and encoded, and now we're ready to use these features for model training and evaluation.

"""

# Save encoded_data to CSV
encoded_data.to_csv('/content/drive/MyDrive/Colab Notebooks/encoded_data.csv', index=False)
print("The file 'encoded_data.csv' has been saved to the Google Drive under 'Colab Notebooks'.")

import pandas as pd
from sklearn.model_selection import train_test_split
from IPython.display import display

# Separate features and target variable (using encoded_data)
X = encoded_data.drop('CHURN', axis=1)  # Drop the target column
y = encoded_data['CHURN']  # Target column

# Check for NaN values in the target variable before splitting
nan_count_before = y.isna().sum()
print(f"Number of NaN values in 'CHURN' before splitting: {nan_count_before}")

# Handle NaN values in the target variable
if nan_count_before > 0:
    encoded_data = encoded_data.dropna(subset=['CHURN'])  # Drop rows where 'CHURN' is NaN
    X = encoded_data.drop('CHURN', axis=1)  # Update X after dropping NaN rows
    y = encoded_data['CHURN']  # Update y after dropping NaN rows

# Recheck NaN values after handling
nan_count_after = y.isna().sum()
print(f"Number of NaN values in 'CHURN' after handling: {nan_count_after}")

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Recheck for NaN values in the split datasets
nan_count_y_train = y_train.isna().sum()
nan_count_y_test = y_test.isna().sum()
print(f"Number of NaN values in 'y_train': {nan_count_y_train}")
print(f"Number of NaN values in 'y_test': {nan_count_y_test}")

# Display the shapes of the datasets
print("Data Splitting Complete")
print("========================")
print(f"Training Features Shape: {X_train.shape}")
print(f"Training Target Shape: {y_train.shape}")
print(f"Testing Features Shape: {X_test.shape}")
print(f"Testing Target Shape: {y_test.shape}")

# Ensure that data types are correct for training
print("\nData Types of Training Features:")
display(pd.DataFrame(X_train.dtypes, columns=['Data Type']))

print("\nData Types of Testing Features:")
display(pd.DataFrame(X_test.dtypes, columns=['Data Type']))

print("\nFirst Few Rows of Training Features:")
display(X_train.head())

print("\nFirst Few Rows of Training Target:")
display(y_train.head())

print("\nFirst Few Rows of Testing Features:")
display(X_test.head())

print("\nFirst Few Rows of Testing Target:")
display(y_test.head())

"""# **3. Train the Model: Methodology**

## **Data Preparation**
- `prepare_data` function to handle non-numeric columns and NaN values consistently.
- Replace NaN values with the median of each column instead of 0, which is often more appropriate.

## **Infinite Value Check**
- A check for infinite values and replace them with a large finite value if found.

## **Model Parameters**
- Reduce the number of estimators to 50 to speed up training.
- Add `n_jobs=-1` to use all available cores, which can speed up training.

## **Progress Tracking**
- Display statements to show progress, which helps identify where the script might be stalling.

## **Feature Importance**
- Limited to showing only the top 20 features to reduce plotting time and memory usage.

## **Error Handling**
- The script handles non-numeric columns by dropping them, which prevents errors if any such columns remain after preprocessing.

"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from IPython.display import display, HTML

def prepare_data(X):
    X = X.apply(pd.to_numeric, errors='coerce')
    non_numeric = X.select_dtypes(exclude=[np.number]).columns
    if len(non_numeric) > 0:
        display(HTML(f"<p>Dropping non-numeric columns: {', '.join(non_numeric)}</p>"))
        X = X.drop(columns=non_numeric)
    X = X.fillna(X.median())
    return X

# Prepare the data
X_train = prepare_data(X_train)
X_test = prepare_data(X_test)

display(HTML("<h3>Data types after preparation:</h3>"))
display(X_train.dtypes)

if np.isinf(X_train).any().any() or np.isinf(X_test).any().any():
    display(HTML("<p>Warning: Infinite values detected. Replacing with large finite values.</p>"))
    X_train = X_train.replace([np.inf, -np.inf], np.finfo(np.float64).max)
    X_test = X_test.replace([np.inf, -np.inf], np.finfo(np.float64).max)

# Initialize and train the model
rf_model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)
display(HTML("<p>Training the model...</p><br><br>"))
rf_model.fit(X_train, y_train)

# Make predictions
display(HTML("<p>Making predictions...</p><br><br>"))
y_pred = rf_model.predict(X_test)
y_prob = rf_model.predict_proba(X_test)[:, 1]

# Accuracy Score
accuracy = accuracy_score(y_test, y_pred)
display(HTML(f"<h3>Accuracy Score: {accuracy:.4f}</h3><br><br>"))

# Classification Report
display(HTML("<h3>Classification Report:</h3><br><br>"))
report = classification_report(y_test, y_pred, output_dict=True)
display(pd.DataFrame(report).transpose())

display(HTML("<br><br>"))  # Add space before the next plot

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Not Churn', 'Churn'], yticklabels=['Not Churn', 'Churn'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
display(plt.gcf())
plt.close()

display(HTML("<br><br>"))  # Add space before the next plot

# ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = roc_auc_score(y_test, y_prob)
plt.figure(figsize=(10, 7))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
display(plt.gcf())
plt.close()

display(HTML("<br><br>"))  # Add space before the next plot

# Feature Importance
importances = rf_model.feature_importances_
features = X_train.columns
importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False).head(20)
plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=importance_df)
plt.title('Top 20 Feature Importance')
display(plt.gcf())
plt.close()

display(HTML("<p>Model training and evaluation completed successfully.</p><br><br>"))

"""# **Streamlit App**"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py

! wget -q -O - ipv4.icanhazip.com

! streamlit run app.py & npx localtunnel --port 8501

"""# **`app.py` Script**"""

import streamlit as st
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Load your preprocessed dataset (encoded_data)
def load_data_and_model():
    encoded_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/encoded_data.csv')

    # Prepare X (features) and y (target)
    X = encoded_data.drop(columns=['CHURN', 'user_id'])  # Drop the target and irrelevant column
    y = encoded_data['CHURN']

    # Split the data into training and testing sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train a Random Forest classifier for demonstration purposes
    clf = RandomForestClassifier(random_state=42)
    clf.fit(X_train, y_train)

    return encoded_data, clf, X.columns

# Initialize the Streamlit app
def main():
    st.title('Churn Prediction App')

    # Load data and model
    churn_dataset, clf, feature_columns = load_data_and_model()

    # Create input fields for each feature
    input_data = {}
    for column in feature_columns:
        # Handle numerical features with number_input
        input_data[column] = st.number_input(f'Enter {column}', value=float(churn_dataset[column].median()))

    # Add a prediction button
    if st.button('Predict Churn'):
        # Convert input data to DataFrame
        input_df = pd.DataFrame([input_data])

        # Make prediction
        prediction = clf.predict(input_df)

        # Display prediction result
        st.write('Prediction:', 'Churn' if prediction[0] == 1 else 'No Churn')

# Run the Streamlit app
if __name__ == '__main__':
    main()